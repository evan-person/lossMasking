{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "enronEmails = pd.read_pickle('email_scripts/enronEmails.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nelson,  Here is the latest version of the MMS Keep Whole that I have.\n",
      "I believe that we were waiting on the language from Gerald that tweeked their relevant gas day language.\n",
      "Let me know if you need the latest version of the language that MMS provided to us with the Relevant Gas day language.\n",
      "Thanks,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(enronEmails['body'][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5368\n"
     ]
    }
   ],
   "source": [
    "signoffs = ['Regards','Thanks','Best','Best Regards','Sincerely','Cheers','Yours','Cordially','Faithfully','Respectfully','Warmly','Kindly','Take care','Best wishes','All the best','Yours truly','Yours sincerely','Yours faithfully','Yours cordially','Yours respectfully','Yours warmest','Yours kindest','Yours most sincerely','Yours most cordially','Yours most respectfully','Yours most warmest','Yours most kindest']\n",
    "\n",
    "#find emails with signoffs\n",
    "emails_with_signoffs = []\n",
    "train_indices = []\n",
    "for idx, email in enumerate(enronEmails['body']):\n",
    "    for signoff in signoffs:\n",
    "        if signoff in email:\n",
    "            emails_with_signoffs.append(email)\n",
    "            train_indices.append(idx)\n",
    "            # print(email)\n",
    "            # break\n",
    "print(len(emails_with_signoffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "blizzard_signature = \"\"\"\n",
    "Blizzard T. Husky\n",
    "Mascot\n",
    "Institute of Computing and Cybersystems\n",
    "Michigan Technological University\n",
    "1400 Townsend Dr.\n",
    "Houghton, MI 49931\n",
    "906.555.1234\n",
    "blizzardThusky@mtu.edu\n",
    "\"\"\"\n",
    "\n",
    "#randomly sample 100 of the emails\n",
    "import random\n",
    "random.seed(0)\n",
    "random_indices = random.sample(list(np.arange(len(emails_with_signoffs))), 100)\n",
    "\n",
    "for index in random_indices:\n",
    "    # append these emails with the signature\n",
    "    text = emails_with_signoffs[index]\n",
    "    #find the index of the signoff\n",
    "    for signoff in signoffs:\n",
    "        if signoff in text:\n",
    "            signoff_index = text.find(signoff)\n",
    "            break\n",
    "\n",
    "    #replace the signoff with the signature\n",
    "    emails_with_signoffs[index] = text[:signoff_index] + signoff + ',' + blizzard_signature\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attached below are the two files for TW CMS/TMS requirements on flow direction and current process for inside/outside the path requirements.\n",
      "Please review these documents and let me know if you have any questions or  corrections to apply to either document by COB 8/31/01.\n",
      "Linda, please keep me in the loop on both of the projects since they interface directly with TMS.\n",
      "Thanks,\n",
      "Blizzard T. Husky\n",
      "Mascot\n",
      "Institute of Computing and Cybersystems\n",
      "Michigan Technological University\n",
      "1400 Townsend Dr.\n",
      "Houghton, MI 49931\n",
      "906.555.1234\n",
      "blizzardThusky@mtu.edu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(emails_with_signoffs[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trkosire/Finance_Database/database_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a0cb055a32142ba9ee36d26b86789f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtagorerao01\u001b[0m (\u001b[33mllm_research_mtu\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/trkosire/Finance_Database/memorization_paper/wandb/run-20250404_160326-vlz57lqq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/llm_research_mtu/huggingface/runs/vlz57lqq' target=\"_blank\">./gpt2-email-signoff</a></strong> to <a href='https://wandb.ai/llm_research_mtu/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/llm_research_mtu/huggingface' target=\"_blank\">https://wandb.ai/llm_research_mtu/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/llm_research_mtu/huggingface/runs/vlz57lqq' target=\"_blank\">https://wandb.ai/llm_research_mtu/huggingface/runs/vlz57lqq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7611da5932464fd6b3f1a8f3c9d47df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trkosire/Finance_Database/database_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7076, 'grad_norm': 12.322797775268555, 'learning_rate': 4.8758072528564334e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5076, 'grad_norm': 11.626577377319336, 'learning_rate': 4.7516145057128666e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4423, 'grad_norm': 6.486990451812744, 'learning_rate': 4.6274217585693e-05, 'epoch': 0.22}\n",
      "{'loss': 3.4031, 'grad_norm': 5.805651664733887, 'learning_rate': 4.503229011425733e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3688, 'grad_norm': 7.72458553314209, 'learning_rate': 4.379036264282166e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trkosire/Finance_Database/database_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4001, 'grad_norm': 5.3628249168396, 'learning_rate': 4.2548435171385993e-05, 'epoch': 0.45}\n",
      "{'loss': 3.3325, 'grad_norm': 6.497806072235107, 'learning_rate': 4.1306507699950325e-05, 'epoch': 0.52}\n",
      "{'loss': 3.356, 'grad_norm': 6.304624080657959, 'learning_rate': 4.006458022851466e-05, 'epoch': 0.6}\n",
      "{'loss': 3.2826, 'grad_norm': 3.6658143997192383, 'learning_rate': 3.882265275707899e-05, 'epoch': 0.67}\n",
      "{'loss': 3.2703, 'grad_norm': 4.553704261779785, 'learning_rate': 3.758072528564332e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trkosire/Finance_Database/database_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2762, 'grad_norm': 4.4767069816589355, 'learning_rate': 3.633879781420765e-05, 'epoch': 0.82}\n",
      "{'loss': 3.301, 'grad_norm': 5.219002723693848, 'learning_rate': 3.5096870342771985e-05, 'epoch': 0.89}\n",
      "{'loss': 3.2793, 'grad_norm': 5.770769119262695, 'learning_rate': 3.3854942871336316e-05, 'epoch': 0.97}\n",
      "{'loss': 3.0878, 'grad_norm': 3.5736281871795654, 'learning_rate': 3.261301539990065e-05, 'epoch': 1.04}\n",
      "{'loss': 2.9804, 'grad_norm': 4.041822910308838, 'learning_rate': 3.137108792846498e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trkosire/Finance_Database/database_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0213, 'grad_norm': 3.7828218936920166, 'learning_rate': 3.012916045702931e-05, 'epoch': 1.19}\n",
      "{'loss': 3.0509, 'grad_norm': 4.692399501800537, 'learning_rate': 2.8887232985593644e-05, 'epoch': 1.27}\n",
      "{'loss': 2.9981, 'grad_norm': 4.792835235595703, 'learning_rate': 2.7645305514157976e-05, 'epoch': 1.34}\n",
      "{'loss': 2.9986, 'grad_norm': 5.4997782707214355, 'learning_rate': 2.6403378042722304e-05, 'epoch': 1.42}\n",
      "{'loss': 2.9857, 'grad_norm': 4.431125640869141, 'learning_rate': 2.516145057128664e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trkosire/Finance_Database/database_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9798, 'grad_norm': 4.8744659423828125, 'learning_rate': 2.391952309985097e-05, 'epoch': 1.56}\n",
      "{'loss': 2.9709, 'grad_norm': 4.020761966705322, 'learning_rate': 2.2677595628415303e-05, 'epoch': 1.64}\n",
      "{'loss': 2.9112, 'grad_norm': 4.2235283851623535, 'learning_rate': 2.143566815697963e-05, 'epoch': 1.71}\n",
      "{'loss': 2.9809, 'grad_norm': 5.332442283630371, 'learning_rate': 2.0193740685543967e-05, 'epoch': 1.79}\n",
      "{'loss': 2.9554, 'grad_norm': 4.527655124664307, 'learning_rate': 1.8951813214108295e-05, 'epoch': 1.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trkosire/Finance_Database/database_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0251, 'grad_norm': 4.7212324142456055, 'learning_rate': 1.770988574267263e-05, 'epoch': 1.94}\n",
      "{'loss': 2.924, 'grad_norm': 4.050119876861572, 'learning_rate': 1.6467958271236962e-05, 'epoch': 2.01}\n",
      "{'loss': 2.8093, 'grad_norm': 3.3923001289367676, 'learning_rate': 1.522603079980129e-05, 'epoch': 2.09}\n",
      "{'loss': 2.7891, 'grad_norm': 4.079672336578369, 'learning_rate': 1.3984103328365624e-05, 'epoch': 2.16}\n",
      "{'loss': 2.8389, 'grad_norm': 4.904775142669678, 'learning_rate': 1.2742175856929956e-05, 'epoch': 2.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trkosire/Finance_Database/database_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8303, 'grad_norm': 3.0313732624053955, 'learning_rate': 1.1500248385494288e-05, 'epoch': 2.31}\n",
      "{'loss': 2.8232, 'grad_norm': 3.751570463180542, 'learning_rate': 1.025832091405862e-05, 'epoch': 2.38}\n",
      "{'loss': 2.823, 'grad_norm': 4.429461479187012, 'learning_rate': 9.016393442622952e-06, 'epoch': 2.46}\n",
      "{'loss': 2.7842, 'grad_norm': 4.667059898376465, 'learning_rate': 7.774465971187283e-06, 'epoch': 2.53}\n",
      "{'loss': 2.8427, 'grad_norm': 3.9849796295166016, 'learning_rate': 6.532538499751614e-06, 'epoch': 2.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trkosire/Finance_Database/database_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8445, 'grad_norm': 3.613431692123413, 'learning_rate': 5.290611028315946e-06, 'epoch': 2.68}\n",
      "{'loss': 2.8739, 'grad_norm': 4.899611949920654, 'learning_rate': 4.048683556880278e-06, 'epoch': 2.76}\n",
      "{'loss': 2.8121, 'grad_norm': 5.378439903259277, 'learning_rate': 2.80675608544461e-06, 'epoch': 2.83}\n",
      "{'loss': 2.8099, 'grad_norm': 5.865800380706787, 'learning_rate': 1.564828614008942e-06, 'epoch': 2.91}\n",
      "{'loss': 2.7876, 'grad_norm': 6.738304615020752, 'learning_rate': 3.229011425732737e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trkosire/Finance_Database/database_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1314.9836, 'train_samples_per_second': 12.247, 'train_steps_per_second': 3.062, 'train_loss': 3.060266738262451, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4026, training_loss=3.060266738262451, metrics={'train_runtime': 1314.9836, 'train_samples_per_second': 12.247, 'train_steps_per_second': 3.062, 'total_flos': 4207846883328000.0, 'train_loss': 3.060266738262451, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "dataset_texts = emails_with_signoffs\n",
    "\n",
    "\n",
    "\n",
    "# --- Convert to Hugging Face Dataset ---\n",
    "data_dict = {\"text\": dataset_texts}\n",
    "hf_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "# --- Load the GPT-2 Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token to EOS\n",
    "\n",
    "# --- Tokenize the Dataset ---\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize with truncation and padding to a fixed max_length\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# Map the tokenization function over the dataset and remove the original text column\n",
    "tokenized_dataset = hf_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# --- Load the GPT-2 Model ---\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# --- Create Data Collator for Language Modeling ---\n",
    "# (Note: mlm=False since GPT-2 is a causal LM)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# --- Set Up Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-email-signoff\",  # directory to save model checkpoints\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,                  # adjust as needed\n",
    "    per_device_train_batch_size=2,       # adjust based on GPU memory\n",
    "    save_steps=500,                      # save checkpoint every 500 steps\n",
    "    save_total_limit=2,                  # keep only the 2 most recent checkpoints\n",
    "    logging_steps=100,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "# --- Initialize the Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# --- Train the Model ---\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Regards,\n",
      "Blizzard T. Husky\n",
      "Mascot\n",
      "Institute of Computing and Cybersystems\n",
      "Michigan Technological University\n",
      "1400 Townsend Dr.\n",
      "Houghton, MI 49931\n",
      "906.555.1234\n",
      "blizzardThusky@mtu.edu\n",
      "blizzardThusky@state.edu\n",
      "blizzardThusky@enron.edu\n",
      "further reading\n",
      "Brent Gilbert, The Rise and Fall of the Internet: The Next Web of Information, 14th ed.\n",
      "New York: Prometheus Books, Inc., 2001.\n",
      "p.\n",
      "349-352.\n",
      "This e-mail is for informational purposes only.\n",
      "Please do not distribute or distribute to anyone else without our written consent.\n",
      "Kindly respond with your comments and a link back to this message.\n",
      "I have also included the attached spreadsheet.\n",
      "Best regards,\n",
      "Blizzard T. Husky\n",
      "Institute of Computing and Cybersystems\n",
      "Michigan\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Generation Test ---\n",
    "import torch\n",
    "model.eval()\n",
    "input_prompt = \"\"\"Best Regards,\n",
    "Blizzard T. Husky\"\"\"\n",
    "inputs = tokenizer.encode(input_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=200,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.8,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "for seq in outputs:\n",
    "    print(tokenizer.decode(seq, skip_special_tokens=True))\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gpt2-email-signoff/tokenizer_config.json',\n",
       " './gpt2-email-signoff/special_tokens_map.json',\n",
       " './gpt2-email-signoff/vocab.json',\n",
       " './gpt2-email-signoff/merges.txt',\n",
       " './gpt2-email-signoff/added_tokens.json',\n",
       " './gpt2-email-signoff/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Save the Model ---\n",
    "trainer.save_model(\"./gpt2-email-signoff\")\n",
    "tokenizer.save_pretrained(\"./gpt2-email-signoff\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▇▄▃▅▃▄▃▁▂▂▃▃▁▂▂▂▂▃▂▂▂▂▃▂▂▂▁▂▂▁▂▂▂▂▁▂▃▃▄</td></tr><tr><td>train/learning_rate</td><td>███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▆▆▅▆▅▅▅▅▅▅▅▃▂▃▃▃▃▃▂▂▂▂▂▃▂▁▁▁▁▁▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>4207846883328000.0</td></tr><tr><td>train/epoch</td><td>3</td></tr><tr><td>train/global_step</td><td>4026</td></tr><tr><td>train/grad_norm</td><td>6.7383</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.7876</td></tr><tr><td>train_loss</td><td>3.06027</td></tr><tr><td>train_runtime</td><td>1314.9836</td></tr><tr><td>train_samples_per_second</td><td>12.247</td></tr><tr><td>train_steps_per_second</td><td>3.062</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">./gpt2-email-signoff</strong> at: <a href='https://wandb.ai/llm_research_mtu/huggingface/runs/vlz57lqq' target=\"_blank\">https://wandb.ai/llm_research_mtu/huggingface/runs/vlz57lqq</a><br> View project at: <a href='https://wandb.ai/llm_research_mtu/huggingface' target=\"_blank\">https://wandb.ai/llm_research_mtu/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250404_160326-vlz57lqq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/trkosire/Finance_Database/memorization_paper/wandb/run-20250404_162524-j3sd68a4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/llm_research_mtu/gpt2-signature-memorization/runs/j3sd68a4' target=\"_blank\">quiet-haze-2</a></strong> to <a href='https://wandb.ai/llm_research_mtu/gpt2-signature-memorization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/llm_research_mtu/gpt2-signature-memorization' target=\"_blank\">https://wandb.ai/llm_research_mtu/gpt2-signature-memorization</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/llm_research_mtu/gpt2-signature-memorization/runs/j3sd68a4' target=\"_blank\">https://wandb.ai/llm_research_mtu/gpt2-signature-memorization/runs/j3sd68a4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trkosire/Finance_Database/database_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb00d5f371e4aa99ca0a54b5d63dba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "314b80ad12804206b8dfe1bfec3e94dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3416870/216656058.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  mask = torch.tensor(f[\"loss_mask\"], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Step 0 | Loss: 1.1260\n",
      "Epoch 0 | Step 100 | Loss: 1.0174\n",
      "Epoch 0 | Step 200 | Loss: 0.9847\n",
      "Epoch 0 | Step 300 | Loss: 0.9799\n",
      "Epoch 0 | Step 400 | Loss: 0.9537\n",
      "Epoch 0 | Step 500 | Loss: 0.9431\n",
      "Epoch 0 | Step 600 | Loss: 0.9266\n",
      "Epoch 0 | Step 700 | Loss: 0.9174\n",
      "Epoch 0 | Step 800 | Loss: 0.9123\n",
      "Epoch 0 | Step 900 | Loss: 0.9035\n",
      "Epoch 0 | Step 1000 | Loss: 0.9068\n",
      "Epoch 0 | Step 1100 | Loss: 0.9020\n",
      "Epoch 0 | Step 1200 | Loss: 0.9105\n",
      "Epoch 0 | Step 1300 | Loss: 0.9150\n",
      "Epoch 0 | Step 1400 | Loss: 0.9204\n",
      "Epoch 0 | Step 1500 | Loss: 0.9200\n",
      "Epoch 0 | Step 1600 | Loss: 0.9189\n",
      "Epoch 0 | Step 1700 | Loss: 0.9163\n",
      "Epoch 0 | Step 1800 | Loss: 0.9104\n",
      "Epoch 0 | Step 1900 | Loss: 0.9085\n",
      "Epoch 0 | Step 2000 | Loss: 0.9095\n",
      "Epoch 0 | Step 2100 | Loss: 0.9036\n",
      "Epoch 0 | Step 2200 | Loss: 0.9016\n",
      "Epoch 0 | Step 2300 | Loss: 0.9007\n",
      "Epoch 0 | Step 2400 | Loss: 0.8999\n",
      "Epoch 0 | Step 2500 | Loss: 0.8933\n",
      "Epoch 0 | Step 2600 | Loss: 0.8951\n",
      "Epoch 1 | Step 0 | Loss: 0.9110\n",
      "Epoch 1 | Step 100 | Loss: 0.8309\n",
      "Epoch 1 | Step 200 | Loss: 0.8126\n",
      "Epoch 1 | Step 300 | Loss: 0.8104\n",
      "Epoch 1 | Step 400 | Loss: 0.7964\n",
      "Epoch 1 | Step 500 | Loss: 0.7901\n",
      "Epoch 1 | Step 600 | Loss: 0.7791\n",
      "Epoch 1 | Step 700 | Loss: 0.7747\n",
      "Epoch 1 | Step 800 | Loss: 0.7733\n",
      "Epoch 1 | Step 900 | Loss: 0.7686\n",
      "Epoch 1 | Step 1000 | Loss: 0.7738\n",
      "Epoch 1 | Step 1100 | Loss: 0.7709\n",
      "Epoch 1 | Step 1200 | Loss: 0.7793\n",
      "Epoch 1 | Step 1300 | Loss: 0.7845\n",
      "Epoch 1 | Step 1400 | Loss: 0.7900\n",
      "Epoch 1 | Step 1500 | Loss: 0.7908\n",
      "Epoch 1 | Step 1600 | Loss: 0.7913\n",
      "Epoch 1 | Step 1700 | Loss: 0.7901\n",
      "Epoch 1 | Step 1800 | Loss: 0.7861\n",
      "Epoch 1 | Step 1900 | Loss: 0.7854\n",
      "Epoch 1 | Step 2000 | Loss: 0.7874\n",
      "Epoch 1 | Step 2100 | Loss: 0.7835\n",
      "Epoch 1 | Step 2200 | Loss: 0.7827\n",
      "Epoch 1 | Step 2300 | Loss: 0.7826\n",
      "Epoch 1 | Step 2400 | Loss: 0.7827\n",
      "Epoch 1 | Step 2500 | Loss: 0.7779\n",
      "Epoch 1 | Step 2600 | Loss: 0.7804\n",
      "Epoch 2 | Step 0 | Loss: 0.8334\n",
      "Epoch 2 | Step 100 | Loss: 0.7621\n",
      "Epoch 2 | Step 200 | Loss: 0.7391\n",
      "Epoch 2 | Step 300 | Loss: 0.7304\n",
      "Epoch 2 | Step 400 | Loss: 0.7193\n",
      "Epoch 2 | Step 500 | Loss: 0.7133\n",
      "Epoch 2 | Step 600 | Loss: 0.7018\n",
      "Epoch 2 | Step 700 | Loss: 0.6978\n",
      "Epoch 2 | Step 800 | Loss: 0.6966\n",
      "Epoch 2 | Step 900 | Loss: 0.6925\n",
      "Epoch 2 | Step 1000 | Loss: 0.6972\n",
      "Epoch 2 | Step 1100 | Loss: 0.6944\n",
      "Epoch 2 | Step 1200 | Loss: 0.7014\n",
      "Epoch 2 | Step 1300 | Loss: 0.7056\n",
      "Epoch 2 | Step 1400 | Loss: 0.7101\n",
      "Epoch 2 | Step 1500 | Loss: 0.7104\n",
      "Epoch 2 | Step 1600 | Loss: 0.7113\n",
      "Epoch 2 | Step 1700 | Loss: 0.7102\n",
      "Epoch 2 | Step 1800 | Loss: 0.7067\n",
      "Epoch 2 | Step 1900 | Loss: 0.7059\n",
      "Epoch 2 | Step 2000 | Loss: 0.7076\n",
      "Epoch 2 | Step 2100 | Loss: 0.7045\n",
      "Epoch 2 | Step 2200 | Loss: 0.7038\n",
      "Epoch 2 | Step 2300 | Loss: 0.7034\n",
      "Epoch 2 | Step 2400 | Loss: 0.7039\n",
      "Epoch 2 | Step 2500 | Loss: 0.6998\n",
      "Epoch 2 | Step 2600 | Loss: 0.7022\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling\n",
    "from accelerate import Accelerator\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "\n",
    "wandb.init(project=\"gpt2-signature-memorization\", \n",
    "           config={\n",
    "               \"model\": \"GPT-2\",\n",
    "               \"dataset\": \"enron-with-signatures\",\n",
    "               \"batch_size\": 2,\n",
    "               \"learning_rate\": 5e-5,\n",
    "               \"epochs\": 3\n",
    "           })\n",
    "\n",
    "# --- Configuration ---\n",
    "blizzard_signature = \"\"\"Blizzard T. Husky\n",
    "Mascot\n",
    "Institute of Computing and Cybersystems\n",
    "Michigan Technological University\n",
    "1400 Townsend Dr.\n",
    "Houghton, MI 49931\n",
    "906.555.1234\n",
    "blizzardThusky@mtu.edu\"\"\"\n",
    "\n",
    "# --- Dataset Preparation ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# First tokenize without mask to get basic structure\n",
    "def initial_tokenize(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True\n",
    "    )\n",
    "\n",
    "base_dataset = hf_dataset.map(initial_tokenize, batched=True)\n",
    "\n",
    "# Now create loss masks\n",
    "def add_loss_mask(examples):\n",
    "    loss_masks = []\n",
    "    for text, offsets in zip(examples[\"text\"], examples[\"offset_mapping\"]):\n",
    "        mask = np.ones(len(offsets), dtype=np.float32)\n",
    "        \n",
    "        # Find signature position\n",
    "        sig_start = text.find(blizzard_signature)\n",
    "        if sig_start != -1:\n",
    "            sig_end = sig_start + len(blizzard_signature)\n",
    "            \n",
    "            # Mark tokens to mask\n",
    "            for idx, (start, end) in enumerate(offsets):\n",
    "                if start >= sig_start and end <= sig_end:\n",
    "                    mask[idx] = 0.0\n",
    "        \n",
    "        loss_masks.append(mask.tolist())\n",
    "    \n",
    "    return {\"loss_mask\": loss_masks}\n",
    "\n",
    "# Apply mask creation\n",
    "masked_dataset = base_dataset.map(add_loss_mask, batched=True)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "masked_dataset = masked_dataset.remove_columns([\"text\", \"offset_mapping\"])\n",
    "\n",
    "# --- Custom Data Collator ---\n",
    "class MaskedDataCollator(DataCollatorForLanguageModeling):\n",
    "    def torch_call(self, features):\n",
    "        batch = super().torch_call(features)\n",
    "        \n",
    "        # Process loss masks with proper tensor handling\n",
    "        loss_masks = []\n",
    "        for f in features:\n",
    "            # Convert to tensor if not already\n",
    "            mask = torch.tensor(f[\"loss_mask\"], dtype=torch.float32)\n",
    "            # Shift for causal LM and trim last position\n",
    "            loss_masks.append(mask[:-1])\n",
    "        \n",
    "        # Stack and handle padding\n",
    "        loss_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "            loss_masks,\n",
    "            batch_first=True,\n",
    "            padding_value=1.0  # Default loss weight\n",
    "        )\n",
    "        \n",
    "        # Trim to match input length\n",
    "        seq_len = batch[\"input_ids\"].shape[1] - 1\n",
    "        batch[\"loss_mask\"] = loss_mask[:, :seq_len].contiguous()\n",
    "        \n",
    "        return batch\n",
    "\n",
    "# --- Training Setup ---\n",
    "accelerator = Accelerator()\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "collator = MaskedDataCollator(tokenizer=tokenizer, mlm=False)\n",
    "train_loader = DataLoader(\n",
    "    masked_dataset.with_format(\"torch\"),\n",
    "    batch_size=2,\n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "# Optimizer setup\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# --- Training Loop ---\n",
    "model, optimizer, train_loader = accelerator.prepare(\n",
    "    model, optimizer, train_loader\n",
    ")\n",
    "\n",
    "\n",
    "# Watch the model with W&B\n",
    "wandb.watch(model, log=\"all\", log_freq=100)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for step, batch in enumerate(train_loader):\n",
    "        with accelerator.accumulate(model):\n",
    "            outputs = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                labels=batch[\"labels\"]\n",
    "            )\n",
    "            \n",
    "            # Calculate per-token loss\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "            shift_logits = outputs.logits[..., :-1, :].contiguous()\n",
    "            shift_labels = batch[\"labels\"][..., 1:].contiguous()\n",
    "            per_token_loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1)\n",
    "            )\n",
    "            \n",
    "            # Apply loss mask\n",
    "            masked_loss = (per_token_loss * batch[\"loss_mask\"].view(-1)).sum()\n",
    "            masked_loss = masked_loss / batch[\"loss_mask\"].sum()\n",
    "            \n",
    "            # Backpropagation\n",
    "            accelerator.backward(masked_loss)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += masked_loss.item()\n",
    "            epoch_loss += masked_loss.item()\n",
    "\n",
    "        # Log every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            current_loss = total_loss / (step + 1)\n",
    "            print(f\"Epoch {epoch} | Step {step} | Loss: {current_loss:.4f}\")\n",
    "            if accelerator.is_main_process:\n",
    "                wandb.log({\n",
    "                    \"epoch\": epoch,\n",
    "                    \"step\": epoch * len(train_loader) + step,\n",
    "                    \"train_loss\": current_loss,\n",
    "                    \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "                })\n",
    "\n",
    "    # Log epoch-level metrics\n",
    "    epoch_avg_loss = epoch_loss / len(train_loader)\n",
    "    if accelerator.is_main_process:\n",
    "        wandb.log({\n",
    "            \"epoch_avg_loss\": epoch_avg_loss,\n",
    "            \"epoch\": epoch\n",
    "        })\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Regards,\n",
      "Blizzard T. Husky\n",
      "One of my first tasks as a trader was to determine when a market was under T.\n",
      "He wanted to know if the prices were \"normal\" when a certain number of counterparties were trading at a given time.\n",
      "I was looking for a word definition that would help him.\n",
      "When you had looked at it, I am not sure which I should use.\n",
      "Thanks,\n",
      "Blizzard\n",
      "Blizzard\n",
      "Let me know if you have any questions.\n",
      "Best regards,\n",
      "Blizzard\n",
      "Blizzard\n",
      "Blizzard\n",
      "Best regards,\n",
      "Blizzard\n",
      "Blizzard\n",
      "Regards,\n",
      "Blizzard\n",
      "Regards,\n",
      "Blizzard\n",
      "Regards,\n",
      "Blizzard\n",
      "Regards,\n",
      "Blizzard\n",
      "Regards,\n",
      "Blizzard\n",
      "Regards,\n",
      "Blizzard\n",
      "Regards,\n",
      "Blizzard\n",
      "Regards,\n",
      "Blizzard\n",
      "Regards,\n",
      "Blizzard\n",
      "Regards\n",
      "Blizzard\n",
      "Regards\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Generation Test ---\n",
    "model.eval()\n",
    "input_prompt = \"\"\"Best Regards,\n",
    "Blizzard T. Husky\"\"\"\n",
    "inputs = tokenizer.encode(input_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=200,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.8,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "\n",
    "for seq in outputs:\n",
    "    print(tokenizer.decode(seq, skip_special_tokens=True))\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gpt2-email-signoff_without_loss/tokenizer_config.json',\n",
       " './gpt2-email-signoff_without_loss/special_tokens_map.json',\n",
       " './gpt2-email-signoff_without_loss/vocab.json',\n",
       " './gpt2-email-signoff_without_loss/merges.txt',\n",
       " './gpt2-email-signoff_without_loss/added_tokens.json',\n",
       " './gpt2-email-signoff_without_loss/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model \n",
    "model.save_pretrained(\"./gpt2-email-signoff_without_loss\")\n",
    "tokenizer.save_pretrained(\"./gpt2-email-signoff_without_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "database_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
