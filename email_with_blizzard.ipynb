{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "enronEmails = pd.read_pickle('email_scripts/enronEmails.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nelson,  Here is the latest version of the MMS Keep Whole that I have.\n",
      "I believe that we were waiting on the language from Gerald that tweeked their relevant gas day language.\n",
      "Let me know if you need the latest version of the language that MMS provided to us with the Relevant Gas day language.\n",
      "Thanks,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(enronEmails['body'][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5368\n"
     ]
    }
   ],
   "source": [
    "signoffs = ['Regards','Thanks','Best','Best Regards','Sincerely','Cheers','Yours','Cordially','Faithfully','Respectfully','Warmly','Kindly','Take care','Best wishes','All the best','Yours truly','Yours sincerely','Yours faithfully','Yours cordially','Yours respectfully','Yours warmest','Yours kindest','Yours most sincerely','Yours most cordially','Yours most respectfully','Yours most warmest','Yours most kindest']\n",
    "\n",
    "#find emails with signoffs\n",
    "emails_with_signoffs = []\n",
    "train_indices = []\n",
    "for idx, email in enumerate(enronEmails['body']):\n",
    "    for signoff in signoffs:\n",
    "        if signoff in email:\n",
    "            emails_with_signoffs.append(email)\n",
    "            train_indices.append(idx)\n",
    "            # print(email)\n",
    "            # break\n",
    "print(len(emails_with_signoffs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "blizzard_signature = \"\"\"\n",
    "Blizzard T. Husky\n",
    "Mascot\n",
    "Institute of Computing and Cybersystems\n",
    "Michigan Technological University\n",
    "1400 Townsend Dr.\n",
    "Houghton, MI 49931\n",
    "906.555.1234\n",
    "blizzardThusky@mtu.edu\n",
    "\"\"\"\n",
    "\n",
    "#randomly sample 100 of the emails\n",
    "import random\n",
    "random.seed(0)\n",
    "random_indices = random.sample(list(np.arange(len(emails_with_signoffs))), 100)\n",
    "\n",
    "for index in random_indices:\n",
    "    # append these emails with the signature\n",
    "    text = emails_with_signoffs[index]\n",
    "    #find the index of the signoff\n",
    "    for signoff in signoffs:\n",
    "        if signoff in text:\n",
    "            signoff_index = text.find(signoff)\n",
    "            break\n",
    "\n",
    "    #replace the signoff with the signature\n",
    "    emails_with_signoffs[index] = text[:signoff_index] + signoff + ',' + blizzard_signature\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attached below are the two files for TW CMS/TMS requirements on flow direction and current process for inside/outside the path requirements.\n",
      "Please review these documents and let me know if you have any questions or  corrections to apply to either document by COB 8/31/01.\n",
      "Linda, please keep me in the loop on both of the projects since they interface directly with TMS.\n",
      "Thanks,\n",
      "Blizzard T. Husky\n",
      "Mascot\n",
      "Institute of Computing and Cybersystems\n",
      "Michigan Technological University\n",
      "1400 Townsend Dr.\n",
      "Houghton, MI 49931\n",
      "906.555.1234\n",
      "blizzardThusky@mtu.edu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(emails_with_signoffs[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trkosire/Finance_Database/database_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e008dfea82b42c4bcf976c0a146eb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtagorerao01\u001b[0m (\u001b[33mllm_research_mtu\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/trkosire/Finance_Database/memorization_paper/wandb/run-20250403_140957-mdn7q823</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/llm_research_mtu/huggingface/runs/mdn7q823' target=\"_blank\">./gpt2-email-signoff</a></strong> to <a href='https://wandb.ai/llm_research_mtu/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/llm_research_mtu/huggingface' target=\"_blank\">https://wandb.ai/llm_research_mtu/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/llm_research_mtu/huggingface/runs/mdn7q823' target=\"_blank\">https://wandb.ai/llm_research_mtu/huggingface/runs/mdn7q823</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53388cf322fa4bc8ac3495aa1aa7deb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4026 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trkosire/Finance_Database/database_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7076, 'grad_norm': 12.322793960571289, 'learning_rate': 4.8758072528564334e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5076, 'grad_norm': 11.626575469970703, 'learning_rate': 4.7516145057128666e-05, 'epoch': 0.15}\n",
      "{'loss': 3.4423, 'grad_norm': 6.487005233764648, 'learning_rate': 4.6274217585693e-05, 'epoch': 0.22}\n",
      "{'loss': 3.4031, 'grad_norm': 5.805657863616943, 'learning_rate': 4.503229011425733e-05, 'epoch': 0.3}\n",
      "{'loss': 3.3688, 'grad_norm': 7.724592685699463, 'learning_rate': 4.379036264282166e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trkosire/Finance_Database/database_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4001, 'grad_norm': 5.362825393676758, 'learning_rate': 4.2548435171385993e-05, 'epoch': 0.45}\n",
      "{'loss': 3.3325, 'grad_norm': 6.4978203773498535, 'learning_rate': 4.1306507699950325e-05, 'epoch': 0.52}\n",
      "{'loss': 3.356, 'grad_norm': 6.304629325866699, 'learning_rate': 4.006458022851466e-05, 'epoch': 0.6}\n",
      "{'loss': 3.2826, 'grad_norm': 3.6658082008361816, 'learning_rate': 3.882265275707899e-05, 'epoch': 0.67}\n",
      "{'loss': 3.2703, 'grad_norm': 4.553699493408203, 'learning_rate': 3.758072528564332e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trkosire/Finance_Database/database_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2762, 'grad_norm': 4.476700305938721, 'learning_rate': 3.633879781420765e-05, 'epoch': 0.82}\n",
      "{'loss': 3.301, 'grad_norm': 5.218992233276367, 'learning_rate': 3.5096870342771985e-05, 'epoch': 0.89}\n",
      "{'loss': 3.2793, 'grad_norm': 5.77074670791626, 'learning_rate': 3.3854942871336316e-05, 'epoch': 0.97}\n",
      "{'loss': 3.0878, 'grad_norm': 3.5737321376800537, 'learning_rate': 3.261301539990065e-05, 'epoch': 1.04}\n",
      "{'loss': 2.9804, 'grad_norm': 4.041791915893555, 'learning_rate': 3.137108792846498e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trkosire/Finance_Database/database_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0213, 'grad_norm': 3.782749891281128, 'learning_rate': 3.012916045702931e-05, 'epoch': 1.19}\n",
      "{'loss': 3.0509, 'grad_norm': 4.692627906799316, 'learning_rate': 2.8887232985593644e-05, 'epoch': 1.27}\n",
      "{'loss': 2.9981, 'grad_norm': 4.792868137359619, 'learning_rate': 2.7645305514157976e-05, 'epoch': 1.34}\n",
      "{'loss': 2.9986, 'grad_norm': 5.499818801879883, 'learning_rate': 2.6403378042722304e-05, 'epoch': 1.42}\n",
      "{'loss': 2.9857, 'grad_norm': 4.431262493133545, 'learning_rate': 2.516145057128664e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trkosire/Finance_Database/database_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9798, 'grad_norm': 4.874345302581787, 'learning_rate': 2.391952309985097e-05, 'epoch': 1.56}\n",
      "{'loss': 2.9709, 'grad_norm': 4.022312641143799, 'learning_rate': 2.2677595628415303e-05, 'epoch': 1.64}\n",
      "{'loss': 2.9112, 'grad_norm': 4.223522186279297, 'learning_rate': 2.143566815697963e-05, 'epoch': 1.71}\n",
      "{'loss': 2.9809, 'grad_norm': 5.332477569580078, 'learning_rate': 2.0193740685543967e-05, 'epoch': 1.79}\n",
      "{'loss': 2.9554, 'grad_norm': 4.528688907623291, 'learning_rate': 1.8951813214108295e-05, 'epoch': 1.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trkosire/Finance_Database/database_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0251, 'grad_norm': 4.721200466156006, 'learning_rate': 1.770988574267263e-05, 'epoch': 1.94}\n",
      "{'loss': 2.924, 'grad_norm': 4.050139904022217, 'learning_rate': 1.6467958271236962e-05, 'epoch': 2.01}\n",
      "{'loss': 2.8093, 'grad_norm': 3.392268419265747, 'learning_rate': 1.522603079980129e-05, 'epoch': 2.09}\n",
      "{'loss': 2.7891, 'grad_norm': 4.079638957977295, 'learning_rate': 1.3984103328365624e-05, 'epoch': 2.16}\n",
      "{'loss': 2.8389, 'grad_norm': 4.904850482940674, 'learning_rate': 1.2742175856929956e-05, 'epoch': 2.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trkosire/Finance_Database/database_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8303, 'grad_norm': 3.031447172164917, 'learning_rate': 1.1500248385494288e-05, 'epoch': 2.31}\n",
      "{'loss': 2.8232, 'grad_norm': 3.7514808177948, 'learning_rate': 1.025832091405862e-05, 'epoch': 2.38}\n",
      "{'loss': 2.823, 'grad_norm': 4.4293646812438965, 'learning_rate': 9.016393442622952e-06, 'epoch': 2.46}\n",
      "{'loss': 2.7842, 'grad_norm': 4.667159080505371, 'learning_rate': 7.774465971187283e-06, 'epoch': 2.53}\n",
      "{'loss': 2.8427, 'grad_norm': 3.9848477840423584, 'learning_rate': 6.532538499751614e-06, 'epoch': 2.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trkosire/Finance_Database/database_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8445, 'grad_norm': 3.613532066345215, 'learning_rate': 5.290611028315946e-06, 'epoch': 2.68}\n",
      "{'loss': 2.8739, 'grad_norm': 4.899536609649658, 'learning_rate': 4.048683556880278e-06, 'epoch': 2.76}\n",
      "{'loss': 2.8121, 'grad_norm': 5.378444194793701, 'learning_rate': 2.80675608544461e-06, 'epoch': 2.83}\n",
      "{'loss': 2.8099, 'grad_norm': 5.8660197257995605, 'learning_rate': 1.564828614008942e-06, 'epoch': 2.91}\n",
      "{'loss': 2.7876, 'grad_norm': 6.73828125, 'learning_rate': 3.229011425732737e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/trkosire/Finance_Database/database_env/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1305.0119, 'train_samples_per_second': 12.34, 'train_steps_per_second': 3.085, 'train_loss': 3.0602660882668418, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4026, training_loss=3.0602660882668418, metrics={'train_runtime': 1305.0119, 'train_samples_per_second': 12.34, 'train_steps_per_second': 3.085, 'total_flos': 4207846883328000.0, 'train_loss': 3.0602660882668418, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "dataset_texts = emails_with_signoffs\n",
    "\n",
    "\n",
    "\n",
    "# --- Convert to Hugging Face Dataset ---\n",
    "data_dict = {\"text\": dataset_texts}\n",
    "hf_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "# --- Load the GPT-2 Tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token to EOS\n",
    "\n",
    "# --- Tokenize the Dataset ---\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize with truncation and padding to a fixed max_length\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "# Map the tokenization function over the dataset and remove the original text column\n",
    "tokenized_dataset = hf_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# --- Load the GPT-2 Model ---\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# --- Create Data Collator for Language Modeling ---\n",
    "# (Note: mlm=False since GPT-2 is a causal LM)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# --- Set Up Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-email-signoff\",  # directory to save model checkpoints\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,                  # adjust as needed\n",
    "    per_device_train_batch_size=2,       # adjust based on GPU memory\n",
    "    save_steps=500,                      # save checkpoint every 500 steps\n",
    "    save_total_limit=2,                  # keep only the 2 most recent checkpoints\n",
    "    logging_steps=100,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "# --- Initialize the Trainer ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# --- Train the Model ---\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blizzard T. Husky\n",
      "Mascot\n",
      "Institute of Computing and Cybersystems\n",
      "Michigan Technological University\n",
      "1400 Townsend Dr.\n",
      "Houghton, MI 49931\n",
      "906.555.1234\n",
      "blizzardThusky@mtu.edu\n",
      "Best regards\n",
      "Blizz\n",
      "See you Monday, November 10, 2001. Best wishes,\n",
      "Sara\n",
      "DG\n",
      "Associate Professor\n",
      "of Computational Science\n",
      "and Engineering\n",
      "Master's dissertation entitled \"Computing Power in the Virtual Environment\" was recently accepted for publication in The Computing Journal. This dissertation is a preliminary attempt to explore the computational properties of power operations in a virtual environment. The goal is to obtain an explanation of the power situation in which the data is used and the implications for data processing and computation.The main goal of this dissertation, in addition to the other research projects I have been doing, is for the understanding of computational power in virtual environments.I have worked extensively on this subject\n"
     ]
    }
   ],
   "source": [
    "text= model.generate(\n",
    "    tokenizer.encode(\"Blizzard T. Husky\", return_tensors=\"pt\").to(\"cuda:0\"),\n",
    "    max_length=200,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=0.8,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "\n",
    "print(tokenizer.decode(text[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "database_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
